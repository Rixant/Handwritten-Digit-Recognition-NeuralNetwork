{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "#import relevent libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "5      0       0       0       0       0       0       0       0       0   \n",
      "6      7       0       0       0       0       0       0       0       0   \n",
      "7      3       0       0       0       0       0       0       0       0   \n",
      "8      5       0       0       0       0       0       0       0       0   \n",
      "9      3       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0         0         0         0   \n",
      "3       0  ...         0         0         0         0         0         0   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "5       0  ...         0         0         0         0         0         0   \n",
      "6       0  ...         0         0         0         0         0         0   \n",
      "7       0  ...         0         0         0         0         0         0   \n",
      "8       0  ...         0         0         0         0         0         0   \n",
      "9       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "5         0         0         0         0  \n",
      "6         0         0         0         0  \n",
      "7         0         0         0         0  \n",
      "8         0         0         0         0  \n",
      "9         0         0         0         0  \n",
      "\n",
      "[10 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# load train and test datasets\n",
    "train=pd.read_csv('datasets/train.csv')\n",
    "print(train.head(10))   \n",
    "test=pd.read_csv('datasets/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of examples : 1 \n",
      "Input nodes : 784 \n"
     ]
    }
   ],
   "source": [
    "#parameters and hyper-parameters defined\n",
    "m=1 #number of training examples\n",
    "print(\"No. of examples : {} \".format(m))\n",
    "num_layer = 3 # number of layers\n",
    "input_nodes = train.shape[1]-1# number of input nodes, excludes first column \n",
    "print(\"Input nodes : {} \".format(input_nodes))\n",
    "h1 = 128      # hidden layer 1\n",
    "h2 = 64       # hidden layer 2\n",
    "output_nodes = 10 # number of output nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=(np.asfarray(train.iloc[0:1,1:]))\n",
    "\n",
    "output = np.zeros((1, 10))\n",
    "for i in range(1):\n",
    "    output[i, np.array(train.label)[i]] = 1.00\n",
    "output = output.astype(float)\n",
    "output=np.transpose(output)\n",
    "input=np.transpose(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of input is 784.\n",
      "The size of output is 10.\n",
      "(784, 1)\n",
      "[[  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [188.]\n",
      " [255.]\n",
      " [ 94.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [191.]\n",
      " [250.]\n",
      " [253.]\n",
      " [ 93.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [123.]\n",
      " [248.]\n",
      " [253.]\n",
      " [167.]\n",
      " [ 10.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 80.]\n",
      " [247.]\n",
      " [253.]\n",
      " [208.]\n",
      " [ 13.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 29.]\n",
      " [207.]\n",
      " [253.]\n",
      " [235.]\n",
      " [ 77.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 54.]\n",
      " [209.]\n",
      " [253.]\n",
      " [253.]\n",
      " [ 88.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 93.]\n",
      " [254.]\n",
      " [253.]\n",
      " [238.]\n",
      " [170.]\n",
      " [ 17.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 23.]\n",
      " [210.]\n",
      " [254.]\n",
      " [253.]\n",
      " [159.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 16.]\n",
      " [209.]\n",
      " [253.]\n",
      " [254.]\n",
      " [240.]\n",
      " [ 81.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 27.]\n",
      " [253.]\n",
      " [253.]\n",
      " [254.]\n",
      " [ 13.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 20.]\n",
      " [206.]\n",
      " [254.]\n",
      " [254.]\n",
      " [198.]\n",
      " [  7.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [168.]\n",
      " [253.]\n",
      " [253.]\n",
      " [196.]\n",
      " [  7.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 20.]\n",
      " [203.]\n",
      " [253.]\n",
      " [248.]\n",
      " [ 76.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 22.]\n",
      " [188.]\n",
      " [253.]\n",
      " [245.]\n",
      " [ 93.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [103.]\n",
      " [253.]\n",
      " [253.]\n",
      " [191.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 89.]\n",
      " [240.]\n",
      " [253.]\n",
      " [195.]\n",
      " [ 25.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 15.]\n",
      " [220.]\n",
      " [253.]\n",
      " [253.]\n",
      " [ 80.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 94.]\n",
      " [253.]\n",
      " [253.]\n",
      " [253.]\n",
      " [ 94.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [ 89.]\n",
      " [251.]\n",
      " [253.]\n",
      " [250.]\n",
      " [131.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [214.]\n",
      " [218.]\n",
      " [ 95.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]]\n",
      "output\n",
      "(10, 1)\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of input is {}.\".format(input_nodes))\n",
    "print(\"The size of output is {}.\".format(output_nodes))\n",
    "print(input.shape)\n",
    "print(input)\n",
    "print(\"output\")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dev_sigmoid(x):\n",
    "    return np.exp(-x)/np.square((1+np.exp(-x)))\n",
    "   \n",
    "\n",
    "def softmax(z):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(z - z.max())\n",
    "        return exps / np.sum(exps, axis=0,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the required parameters with array of appropriate dimensions\n",
    "def parameters_initialization(input_nodes, h1, h2, output_nodes):\n",
    "    \n",
    "    #weights are initialized in each layer with random values\n",
    "    W1=np.random.randn(h1,input_nodes)*0.1\n",
    "    b1=np.zeros((h1,1))\n",
    "    W2=np.random.randn(h2, h1)*np.sqrt(1./h2)\n",
    "    b2=np.zeros((h2,1))\n",
    "    W3=np.random.randn(output_nodes, h2)*0.01\n",
    "    b3=np.zeros((output_nodes,1))\n",
    "    \n",
    "    \n",
    "    parameters={\n",
    "        \"W1\":W1,\n",
    "        \"W2\":W2,\n",
    "        \"W3\":W3,\n",
    "        \"b1\":b1,\n",
    "        \"b2\":b2,\n",
    "        \"b3\":b3\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16712796 -0.01948439  0.06902172 ... -0.14852273  0.04772558\n",
      "  -0.09262205]\n",
      " [-0.06547296 -0.02994775  0.03295105 ... -0.14476575  0.16343879\n",
      "   0.00893111]\n",
      " [-0.01847978 -0.09366893  0.09179814 ...  0.14030117 -0.05251656\n",
      "  -0.30819514]\n",
      " ...\n",
      " [ 0.07166184 -0.04526474 -0.18173908 ...  0.06307818  0.1437201\n",
      "  -0.07388253]\n",
      " [-0.04891401 -0.00481998  0.02595821 ...  0.15093765  0.09302974\n",
      "  -0.07223962]\n",
      " [-0.0084853  -0.11944831 -0.02604414 ...  0.03071941  0.05661754\n",
      "   0.03807561]]\n",
      "[[ 0.05874683  0.01580867  0.11115811 ...  0.06692281  0.12234527\n",
      "   0.08537525]\n",
      " [-0.15832211 -0.23533382 -0.06067673 ...  0.06172121  0.09802336\n",
      "   0.12969035]\n",
      " [ 0.21705273 -0.09804    -0.09171677 ... -0.11856733 -0.00034855\n",
      "  -0.03711531]\n",
      " ...\n",
      " [ 0.02616441  0.00328123 -0.09839498 ...  0.00973021  0.01075333\n",
      "   0.01738932]\n",
      " [ 0.1622164   0.1160691  -0.0015821  ... -0.04502873 -0.06055448\n",
      "   0.09342186]\n",
      " [-0.00842336 -0.14035765 -0.0500053  ...  0.22138361  0.09025002\n",
      "  -0.18752534]]\n",
      "[[ 6.44386760e-04 -5.27389625e-03  3.92071529e-03 -6.67843021e-04\n",
      "  -1.75486876e-02  3.72261264e-03 -4.94018666e-04  1.92318164e-03\n",
      "   2.23812268e-02  3.24945644e-03 -4.24245407e-03  1.25979859e-02\n",
      "  -5.46348585e-03  8.11502566e-04  3.11349263e-04  7.54339577e-03\n",
      "   1.17281121e-02 -1.82122104e-02 -1.62345117e-03  6.75230841e-03\n",
      "   3.89718066e-03  6.29668460e-03 -1.16585633e-02 -9.03313727e-03\n",
      "   7.14852689e-03 -1.41142174e-03 -3.61035474e-03  7.36775023e-03\n",
      "   1.23435065e-03  6.83406554e-03 -2.46165942e-02  1.09276155e-02\n",
      "   1.28908506e-03  1.01371441e-02 -1.18148262e-02 -1.48010431e-02\n",
      "   1.69971515e-02 -1.64218932e-02 -9.50776923e-03 -2.91105604e-03\n",
      "  -8.52282024e-04 -1.18599135e-02 -7.41262603e-03 -8.27447752e-03\n",
      "   9.62108554e-03  1.07908057e-02  1.64388625e-02  5.57068846e-03\n",
      "   3.36822931e-03  1.48175588e-03  4.35664717e-03 -1.31289923e-02\n",
      "  -6.19348177e-03  1.41056859e-02 -4.14580970e-03  2.29175410e-03\n",
      "  -1.70655678e-02 -2.67681959e-03 -9.84465220e-03 -9.67235236e-03\n",
      "   1.09544269e-02 -1.14380763e-02 -5.66163050e-03 -8.74441020e-04]\n",
      " [ 6.69041254e-03  1.85293029e-03  1.90642142e-02 -5.60459151e-03\n",
      "   2.08507724e-05 -2.96281919e-03 -2.34621523e-02  1.08070962e-03\n",
      "   2.13427741e-02 -9.41870848e-04 -6.66271219e-03  9.53404555e-03\n",
      "  -8.33547460e-03 -9.91645436e-03  3.86826862e-03  1.35546310e-02\n",
      "   3.05468891e-03  1.34673187e-02  8.96992435e-03 -1.21511774e-03\n",
      "  -2.23046897e-02  3.03134806e-03 -6.71853355e-03 -2.23187891e-02\n",
      "   2.07583504e-02 -2.05016149e-03 -9.13682943e-03 -5.16803006e-03\n",
      "  -7.65928175e-03  2.59322198e-03 -1.94538192e-03  1.21681496e-02\n",
      "  -1.00159065e-02 -7.41107924e-03  1.08900310e-02  5.31951588e-03\n",
      "   1.45634124e-02  2.02141082e-02  5.93364908e-03 -1.67670431e-02\n",
      "   8.13870415e-03 -1.90998053e-02 -7.95669338e-03 -3.57964957e-04\n",
      "  -7.09356579e-03 -1.35124120e-02  5.94751401e-03 -2.94160234e-03\n",
      "   3.07819627e-03  6.50003775e-03  3.10028817e-03 -6.29250425e-03\n",
      "  -2.64865164e-02  5.40305477e-03  4.79918712e-03 -1.45037877e-04\n",
      "   1.80001994e-02  1.32220171e-03  5.80784927e-03 -4.45588776e-04\n",
      "  -3.21285976e-03  8.11289592e-03  5.74424646e-04  2.94494625e-03]\n",
      " [ 2.48479796e-02 -9.91398549e-03  6.29619182e-03 -6.17608255e-03\n",
      "  -1.40457003e-02  4.84422237e-03  1.90658003e-02 -1.05419697e-02\n",
      "  -2.34110617e-03  7.37374060e-03 -8.12445316e-03  1.75296107e-02\n",
      "   1.13739084e-02 -7.19544052e-04 -3.87554724e-03 -1.13301280e-03\n",
      "   2.25281288e-02  2.35620536e-03 -2.03994312e-02  9.40137914e-03\n",
      "  -7.01996365e-03 -1.94114235e-03  7.28541063e-03 -4.80741301e-03\n",
      "   4.70266217e-03  1.74935702e-02 -3.67602551e-03  4.56331678e-03\n",
      "   1.06563100e-02  3.94660619e-03 -9.57129228e-03 -1.22578744e-02\n",
      "  -1.49467570e-03  6.51298919e-03 -2.15818778e-02 -1.25222159e-02\n",
      "  -2.96227847e-02 -1.44919829e-02  1.23826014e-02  6.61793929e-03\n",
      "   2.47793534e-03 -1.95232657e-03  3.02034709e-03 -4.43341353e-03\n",
      "   1.30775483e-03  1.71480137e-02  7.24790170e-03 -2.64767628e-02\n",
      "   4.52878750e-03  1.00426747e-02  1.63346500e-02 -1.22872225e-02\n",
      "   1.20431707e-02 -8.71121657e-03 -1.29608046e-02  4.45581030e-03\n",
      "  -1.12478679e-02 -9.13287046e-03 -5.56525306e-03 -4.92931175e-03\n",
      "  -3.90655406e-03  2.72057964e-03  5.98328897e-03  7.22490382e-03]\n",
      " [ 9.82255200e-06 -1.30753068e-03 -4.11693212e-03  2.16861012e-03\n",
      "  -2.44118111e-03  1.00711441e-02 -7.05313389e-04  5.34409875e-03\n",
      "   5.64169487e-03  1.40195003e-02 -7.46564921e-03 -1.04185993e-02\n",
      "  -7.23944414e-03  1.21715106e-02 -4.56783002e-03 -2.49841367e-03\n",
      "  -8.69184051e-03 -4.64624355e-03 -7.48312890e-03 -2.02783400e-04\n",
      "  -2.73224971e-02  9.14573218e-03 -5.63190751e-03 -2.10880014e-03\n",
      "  -1.08524859e-02  2.02120632e-02 -1.70756927e-02 -9.38960942e-03\n",
      "   4.54139667e-03 -8.88089172e-03  6.42457089e-03 -4.91507535e-03\n",
      "   2.08310474e-02  1.01877709e-03 -5.42855448e-03  9.75056633e-03\n",
      "  -9.02793955e-03 -1.36407289e-03  1.14610260e-02  5.92755661e-03\n",
      "   1.06625981e-02  9.51944688e-03 -1.37418096e-02  5.64658847e-03\n",
      "   1.02273770e-02  1.09461794e-02 -9.41069242e-04  5.84005444e-03\n",
      "   3.88272591e-03 -1.45655282e-03  1.96906729e-03  1.39572095e-02\n",
      "   4.20059378e-03  2.62614734e-02 -1.53556190e-03 -3.27332432e-03\n",
      "   9.19872148e-03 -1.44067083e-03 -5.48885867e-03  2.90498386e-03\n",
      "   3.02154539e-03  5.01860911e-04 -1.44782208e-02 -9.27661849e-04]\n",
      " [ 1.18726903e-03 -8.17798311e-03 -8.12818742e-03 -4.13099372e-03\n",
      "  -1.38254120e-02  6.95773290e-03  2.74391942e-03  9.77955214e-03\n",
      "   4.13314565e-03 -6.71593698e-03 -1.63372540e-02 -2.19275128e-02\n",
      "   5.48385657e-03  2.49691527e-03  7.10495300e-03 -2.53232908e-03\n",
      "   7.59680430e-03 -1.02894313e-02  1.13187161e-02 -3.76310935e-03\n",
      "   1.08788534e-02 -9.43495707e-03  2.38356642e-03 -5.15731730e-04\n",
      "   2.79266635e-03 -1.55414239e-02 -6.44998947e-03  6.14424775e-03\n",
      "  -9.79995692e-03 -1.07868063e-02 -8.35799396e-04 -8.66224925e-03\n",
      "   1.02574359e-02 -1.60758323e-02 -1.77216453e-02 -3.06161476e-03\n",
      "  -1.13195928e-02  6.31324053e-03  7.57116315e-03  1.26838311e-02\n",
      "   4.50370643e-03 -1.22381917e-02  8.40392539e-03 -2.00290817e-02\n",
      "  -4.27187408e-03  2.49951676e-02  4.94784496e-03  4.72237313e-04\n",
      "   9.60097465e-03  1.87605977e-02 -5.37321346e-03  2.18863686e-03\n",
      "  -1.20805039e-02  1.61111752e-02 -1.08229741e-02 -1.23882711e-02\n",
      "  -1.33849007e-02  5.64159911e-03 -1.63483583e-02  7.20910937e-03\n",
      "  -1.31596935e-02 -1.31172078e-02 -1.18665043e-02 -7.54282005e-03]\n",
      " [-6.85504229e-03  4.62627659e-03 -6.47958189e-03  1.38030218e-03\n",
      "  -7.29439425e-03 -1.15122550e-02 -1.38558572e-02 -4.08580518e-03\n",
      "  -1.07417250e-02  1.04664455e-02 -2.09620689e-02  3.49678419e-03\n",
      "   3.75325838e-03 -1.30376824e-02 -1.65741514e-02  6.23416805e-03\n",
      "  -8.21809525e-03  8.92394093e-03  4.83555487e-03 -2.19028063e-03\n",
      "   2.10630396e-02 -6.04142704e-03 -3.09484983e-03 -3.91552744e-04\n",
      "  -1.21758046e-03  2.90049991e-03  2.13126071e-03 -7.99593871e-04\n",
      "   1.60910140e-02 -2.61679944e-03  1.13421233e-02  1.35010528e-04\n",
      "   6.08690602e-03 -7.16972909e-03  7.63468535e-03 -6.02296567e-03\n",
      "  -1.90704617e-03 -2.92498449e-03 -7.62358562e-03 -4.60135503e-03\n",
      "   2.54646133e-03 -1.32804068e-02  4.56795277e-03 -3.12617923e-03\n",
      "  -8.97259666e-03  2.53205821e-03 -1.39023915e-02 -3.62742273e-03\n",
      "  -1.41047272e-02 -3.15976500e-03 -5.66532142e-03  4.99509410e-04\n",
      "   1.59580215e-02 -2.71801227e-03 -1.15480012e-02  1.70827632e-03\n",
      "  -7.28897575e-03 -1.54736160e-02 -4.06092801e-03 -8.50274672e-03\n",
      "   1.72233966e-02  9.16065572e-03  1.39331230e-03  1.65825110e-02]\n",
      " [-3.66662124e-03 -4.46423440e-03  5.82967378e-03  8.32857029e-03\n",
      "   3.56084332e-03  9.54832045e-03  1.55865161e-02 -6.02475170e-03\n",
      "  -7.89326022e-03  3.27478817e-04  4.80473508e-03  2.40177607e-02\n",
      "   9.26681570e-03 -7.41183719e-03  1.58886341e-02  6.48226527e-03\n",
      "   7.90745823e-03 -2.05221615e-02  1.38385308e-02 -4.07696009e-03\n",
      "  -9.27072223e-03 -1.59605072e-02 -7.21942295e-03 -3.99375177e-04\n",
      "   7.83692015e-03 -6.79600207e-03  4.86749421e-03  1.34729775e-03\n",
      "   2.55148255e-02 -1.58071452e-02  8.26111879e-03  1.46410063e-02\n",
      "   1.72054066e-03  1.96078602e-02  1.55009809e-02  3.08811938e-03\n",
      "  -4.60091457e-04 -2.20711448e-03  6.50709404e-03 -1.16510393e-03\n",
      "  -1.00176129e-02  4.32057424e-03 -9.56979904e-03  8.33560524e-03\n",
      "  -2.58179605e-03  2.29297729e-03  7.20022822e-03  6.82192219e-03\n",
      "   1.29117737e-03  5.41540361e-03 -6.64999283e-03 -2.27418214e-02\n",
      "  -8.47706823e-03  5.00705172e-03 -2.92980264e-04 -4.40031058e-03\n",
      "   6.18201233e-03 -2.63021392e-03 -1.06204548e-03  9.68438161e-03\n",
      "  -6.94175027e-03  1.51911392e-03 -4.14870595e-03 -1.13335207e-03]\n",
      " [-1.42856535e-02  7.14038921e-03 -1.50168401e-02 -3.78346873e-03\n",
      "   5.43913038e-03  1.50942872e-02 -5.17556942e-03 -9.17377941e-03\n",
      "   4.71761778e-04  1.53200535e-02  3.43097044e-03  5.13380182e-03\n",
      "  -1.19171807e-02  2.42394044e-02 -6.37330897e-03 -1.37758126e-02\n",
      "   7.67900237e-03  3.89010710e-03 -9.88124830e-03  7.33900319e-03\n",
      "   1.98108648e-03 -1.17760894e-02 -7.22996324e-03 -7.00722290e-03\n",
      "  -1.05151521e-02 -2.22859854e-02 -8.98538064e-03 -6.92344236e-03\n",
      "   4.15462097e-03 -6.54269701e-03  7.39429470e-03  2.79880894e-03\n",
      "  -9.53718044e-03 -8.95866165e-03 -1.34822410e-02  1.48690266e-03\n",
      "  -1.83036544e-02 -1.03436926e-02 -1.57342486e-02 -1.02371595e-02\n",
      "  -2.22059792e-03 -3.13425348e-03  2.84723817e-03 -2.04041799e-02\n",
      "  -1.67000742e-02 -1.98720322e-02 -9.19191749e-03 -9.76964293e-04\n",
      "   1.50725053e-02  1.44701786e-02  1.64829694e-03  1.19991899e-02\n",
      "  -1.44274494e-02 -1.55066710e-02  1.39493918e-04 -8.39097732e-04\n",
      "  -6.97000398e-03 -8.84164188e-03  4.20977451e-03 -1.52850614e-03\n",
      "  -1.19678924e-02  7.53642561e-03 -2.23368637e-02 -3.08303692e-02]\n",
      " [-8.49305277e-03 -1.10749878e-02  5.54818822e-03  2.75760388e-03\n",
      "  -3.17719253e-02 -5.22452554e-03 -1.46901723e-02  1.31442843e-02\n",
      "   6.72789822e-03  1.48638977e-02 -8.87144061e-03 -1.11279307e-02\n",
      "  -8.13632831e-03 -2.07379368e-02  1.80546468e-02  1.72924661e-02\n",
      "   1.56728992e-02 -1.03639020e-02  5.65032601e-03  1.17959067e-02\n",
      "  -1.06191487e-02  7.97707537e-03 -5.26195039e-03  5.05237432e-03\n",
      "   1.16421903e-03 -9.63393260e-03 -1.30974657e-02  2.00867178e-03\n",
      "   4.14681433e-03 -2.82020559e-03 -5.93182042e-03  2.43765358e-03\n",
      "   7.08487483e-03 -5.42398674e-03  1.08938082e-02  5.59702550e-03\n",
      "   3.71679148e-03  5.92615148e-03  4.23822678e-03 -1.00458957e-02\n",
      "  -8.23665730e-03 -1.49421538e-03  7.06276303e-03  2.32983228e-02\n",
      "  -1.91325218e-03  2.28416640e-04  9.10694147e-03  1.13976238e-02\n",
      "   1.04578243e-02 -1.54849893e-02 -5.00930945e-03  1.48616936e-02\n",
      "  -2.16378796e-03  1.42626507e-03 -4.33074199e-03  1.54560868e-02\n",
      "   1.94175953e-04 -1.15333663e-02  1.72431582e-03 -9.25444153e-04\n",
      "  -1.98227506e-02  1.74932486e-02 -2.35391436e-03 -2.06040088e-03]\n",
      " [ 7.68986282e-03  1.02861236e-02  2.39088890e-04 -5.92014148e-03\n",
      "  -1.84247200e-02  7.97508075e-03 -1.12782162e-02  1.05938740e-02\n",
      "   1.16164440e-02  1.78514778e-02 -1.70866121e-03  7.44031697e-03\n",
      "  -1.87166677e-03  1.34139725e-02  2.21932856e-03  8.90461127e-04\n",
      "   2.34214088e-03 -1.27198991e-03  6.47559236e-03  3.62575755e-03\n",
      "  -4.03340621e-03 -4.33972422e-03  2.26672527e-02 -6.29986337e-05\n",
      "   6.46491192e-03  2.89892473e-03  1.45432816e-03 -1.27399021e-03\n",
      "   1.41654459e-02 -1.06151319e-02  1.37323855e-03 -4.98732891e-03\n",
      "  -1.36630800e-02 -6.46312888e-03  9.42995450e-03  9.21654060e-03\n",
      "  -6.87468947e-04 -1.45387490e-02  4.08670270e-03 -1.64536493e-02\n",
      "  -9.39287640e-03 -1.27217581e-02 -2.70158819e-03 -8.94577422e-03\n",
      "   4.51946724e-03  1.01313255e-02 -1.32923660e-02 -7.91546499e-03\n",
      "   2.97596533e-04  3.27200187e-03  5.50019656e-03  9.29452225e-03\n",
      "   6.12070630e-03  3.52235580e-03 -8.84689512e-05  3.15210075e-03\n",
      "   1.77896236e-02  6.07025641e-03 -5.49882647e-04 -5.88989111e-03\n",
      "   1.25271681e-03 -1.05168962e-03  5.95395923e-03  6.55550949e-04]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters=parameters_initialization(input_nodes, h1, h2, output_nodes)\n",
    "print(parameters['W1'])\n",
    "print(parameters[\"W2\"])\n",
    "print(parameters[\"W3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(input,parameters):\n",
    "    \n",
    "    \n",
    "    Z1=np.dot(parameters[\"W1\"],input)+parameters[\"b1\"]\n",
    "    A1=np.tanh(Z1)\n",
    "    Z2=np.dot(parameters[\"W2\"],A1)+parameters[\"b2\"]\n",
    "    A2=np.tanh(Z2)\n",
    "    Z3=np.dot(parameters[\"W3\"],A2)+parameters[\"b3\"]\n",
    "    A3=sigmoid(Z3)\n",
    "    print(A3.shape)\n",
    "    \n",
    "    cache={\n",
    "        \"A1\": A1,\n",
    "        \"A2\": A2,\n",
    "        \"A3\": A3,\n",
    "        \"Z1\": Z1,\n",
    "        \"Z2\": Z2,\n",
    "        \"Z3\": Z3\n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "    return A3, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "[[0.51044009]\n",
      " [0.4942475 ]\n",
      " [0.51798403]\n",
      " [0.51276997]\n",
      " [0.48370985]\n",
      " [0.50212736]\n",
      " [0.49968567]\n",
      " [0.51361109]\n",
      " [0.49956903]\n",
      " [0.5114585 ]]\n"
     ]
    }
   ],
   "source": [
    "A3, cache=forward_propagation(input,parameters)\n",
    "print(cache[\"A3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, output, parameters):\n",
    "    logprobs = np.multiply(np.log(A2),output)\n",
    "    cost = -np.sum(logprobs)/output.shape[1]\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(input,output,learning_rate,parameters, cache):\n",
    " \n",
    "    dZ3=cache[\"A3\"]-output\n",
    "    dW3=np.dot(dZ3,cache[\"A2\"].T)/784\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims = True)/784\n",
    "   \n",
    "    \n",
    "    dZ2=np.dot(parameters[\"W3\"].T,dZ3)*(1 - np.power(cache[\"A2\"], 2))\n",
    "    dW2=np.dot(dZ2,cache[\"A1\"].T)/784\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims = True)/784\n",
    "    \n",
    "    \n",
    "    dZ1=np.dot(parameters[\"W2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n",
    "    dW1=np.dot(dZ1,input.T)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims = True)/784\n",
    "    \n",
    "    \n",
    "    \n",
    "    grads={\n",
    "        'dW1':dW1,\n",
    "        'dW2':dW2,\n",
    "        'dW3':dW3,\n",
    "        'db1':db1,\n",
    "        'db2':db2,\n",
    "        'db3':db3\n",
    "    }\n",
    "    \n",
    "\n",
    "    return grads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads):\n",
    "    \n",
    "    W1 = parameters[\"W1\"]-learning_rate*grads[\"dW1\"]\n",
    "    b1 = parameters[\"b1\"]-learning_rate*grads[\"db1\"]\n",
    "    W2 = parameters[\"W2\"]-learning_rate*grads[\"dW2\"]\n",
    "    b2 = parameters[\"b2\"]-learning_rate*grads[\"db2\"]\n",
    "    W3 = parameters[\"W3\"]-learning_rate*grads[\"dW3\"]\n",
    "    b3 = parameters[\"b3\"]-learning_rate*grads[\"db3\"]\n",
    "\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b1\": b1,\n",
    "                  \"b2\": b2,\n",
    "                  \"b3\": b3\n",
    "                }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2 - Layer neural network\n",
    "# def neural_network_model(x_train, y_train,x_test,y_test, num_iterations):\n",
    "#     cost_list = []\n",
    "#     index_list = []\n",
    "#     #initialize parameters and layer sizes\n",
    "#     parameters = parameters_initialization(x_train, y_train)\n",
    "\n",
    "#     for i in range(0, num_iterations):\n",
    "#          # forward propagation\n",
    "#         A2, cache = forward_propagation(x_train,parameters)\n",
    "#         # compute cost\n",
    "#         cost = compute_cost(A2, y_train, parameters)\n",
    "#          # backward propagation\n",
    "#         grads = backward_propagatio(parameters, cache, x_train, y_train)\n",
    "#          # update parameters\n",
    "#         parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "#         if i % 100 == 0:\n",
    "#             cost_list.append(cost)\n",
    "#             index_list.append(i)\n",
    "#             print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "#     plt.plot(index_list,cost_list)\n",
    "#     plt.xticks(index_list,rotation='vertical')\n",
    "#     plt.xlabel(\"Number of Iterarion\")\n",
    "#     plt.ylabel(\"Cost\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     # predict\n",
    "#     y_prediction_test = predict(parameters,x_test)\n",
    "#     y_prediction_train = predict(parameters,x_train)\n",
    "\n",
    "#     # Print train/test Errors\n",
    "#     print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "#     print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "#     return parameters\n",
    "\n",
    "# parameters =neural_network_model(x_train, y_train,x_test,y_test, num_iterations=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.52198127]\n",
      " [0.50976384]\n",
      " [0.50875043]\n",
      " [0.4782411 ]\n",
      " [0.51839188]\n",
      " [0.50800441]\n",
      " [0.49490561]\n",
      " [0.49348427]\n",
      " [0.48407858]\n",
      " [0.49293698]]\n",
      "Epoch: 2/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.52166946]\n",
      " [0.50981393]\n",
      " [0.50868861]\n",
      " [0.47815237]\n",
      " [0.51836311]\n",
      " [0.50800171]\n",
      " [0.49461881]\n",
      " [0.49349442]\n",
      " [0.48401105]\n",
      " [0.49292703]]\n",
      "Epoch: 3/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.52161207]\n",
      " [0.50986732]\n",
      " [0.50863253]\n",
      " [0.47810102]\n",
      " [0.51830688]\n",
      " [0.5079461 ]\n",
      " [0.49456348]\n",
      " [0.49343959]\n",
      " [0.48395826]\n",
      " [0.49287294]]\n",
      "Epoch: 4/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.52155203]\n",
      " [0.50992155]\n",
      " [0.50857297]\n",
      " [0.47805363]\n",
      " [0.51825176]\n",
      " [0.50789001]\n",
      " [0.49450211]\n",
      " [0.49338048]\n",
      " [0.48390549]\n",
      " [0.49281692]]\n",
      "Epoch: 5/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.51968518]\n",
      " [0.5102567 ]\n",
      " [0.50718114]\n",
      " [0.47994525]\n",
      " [0.51829438]\n",
      " [0.50739871]\n",
      " [0.49223559]\n",
      " [0.49199842]\n",
      " [0.48439187]\n",
      " [0.49248456]]\n",
      "Epoch: 6/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.5176465 ]\n",
      " [0.51047589]\n",
      " [0.5062263 ]\n",
      " [0.48156969]\n",
      " [0.51790398]\n",
      " [0.5068225 ]\n",
      " [0.49103117]\n",
      " [0.49146922]\n",
      " [0.48520503]\n",
      " [0.49268144]]\n",
      "Epoch: 7/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.51758834]\n",
      " [0.51053063]\n",
      " [0.50616986]\n",
      " [0.48151578]\n",
      " [0.51784642]\n",
      " [0.5067647 ]\n",
      " [0.49097556]\n",
      " [0.49141319]\n",
      " [0.48515082]\n",
      " [0.49262694]]\n",
      "Epoch: 8/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.51752684]\n",
      " [0.51058876]\n",
      " [0.50611362]\n",
      " [0.48145959]\n",
      " [0.51779009]\n",
      " [0.50669732]\n",
      " [0.49091509]\n",
      " [0.4913491 ]\n",
      " [0.48509596]\n",
      " [0.49257603]]\n",
      "Epoch: 9/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.51560923]\n",
      " [0.51200462]\n",
      " [0.50604874]\n",
      " [0.48085477]\n",
      " [0.51775387]\n",
      " [0.50226615]\n",
      " [0.48896975]\n",
      " [0.48860833]\n",
      " [0.48524561]\n",
      " [0.49430306]]\n",
      "Epoch: 10/10\n",
      "(10, 1)\n",
      "A3\n",
      "[[0.51553215]\n",
      " [0.51207013]\n",
      " [0.50598962]\n",
      " [0.48079816]\n",
      " [0.51769099]\n",
      " [0.50217112]\n",
      " [0.48889968]\n",
      " [0.4885367 ]\n",
      " [0.48519609]\n",
      " [0.49426273]]\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "parameters=parameters_initialization(input_nodes, h1, h2, output_nodes)\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \"+ str(epoch+1)+\"/\"+str(epochs))   \n",
    "    A3,cache=forward_propagation(input,parameters)\n",
    "    grads=back_propagation(input,output,learning_rate,parameters,cache)\n",
    "    parameters=update_parameters(parameters,grads)\n",
    "    print(\"A3\")\n",
    "    print(cache[\"A3\"])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x =test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
